{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning notes of 深度学习进阶：自然语言处理\n",
    "# ch4： word2vec的加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "# cW_in = h\n",
    "import numpy as np\n",
    "\n",
    "class Embedding:\n",
    "    '''\n",
    "    Embedding层本质上是把传入的c（one-hot）表示的行取出来，取出来的的行就是h，也即隐藏层神经元\n",
    "    '''\n",
    "    def __inti__(self, W):\n",
    "        # prams grads是成员变量，idx用于提取params和grads的行\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "\n",
    "        # 保持dW形状不变，将dW的所有元素变成0\n",
    "        dW[...] = 0\n",
    "        # dW[self.idx] = dout\n",
    "\n",
    "        # 加法是为了处理self.idx中可能出现多个同样的值\n",
    "        for i, word_id in enumerate(self.idx):\n",
    "            dW[word_id] += dout[i]\n",
    "        # or\n",
    "        # np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dot 层：h->(embedding dot)-> W_out -> sigmoid with loss (不是softmax with loss)\n",
    "# 看书上p147图4-14\n",
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        #成员变量\n",
    "        # 这里的W是W_out，也即输出层的参数矩阵\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        # idx是一个列表（ndarray），因为我们都是处理minibatch，不会只处理一个单词\n",
    "        # target_W 是实际参与输出层计算的输出层参数矩阵的列（负采样的策略：只计算目标词代表的列向量）\n",
    "        target_W = self.embed.forward(idx)\n",
    "        #沿着axis=1（沿着行加）加\n",
    "        out = np.sum(target_W*h, axis = 1)\n",
    "        # self.cache保存正向传播的计算结果\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout*h\n",
    "\n",
    "        # backward更新输出层参数W_out\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout*target_W\n",
    "        #embedding层的backward不会返回任何东西，但是embeddingdot层因为有内积运算，所以会返回dh\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于概率分布的负采样：出现频率越高的词越有可能被采样\n",
    "GPU = False # 在config中设定：from common.config import GPU\n",
    "import collections\n",
    "class UnigramSampler:\n",
    "    # power: <1 平滑处理，让低频的词没有那么难被抽到\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "        \n",
    "        #原始代码的实现方式（低效）\n",
    "        # counts = collections.Counter()\n",
    "        # for word_id in corpus:\n",
    "        #     counts[word_id] += 1\n",
    "        counts = collections.Counter(corpus)\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        if not GPU:\n",
    "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                p = self.word_p.copy()\n",
    "                target_idx = target[i]\n",
    "                p[target_idx] = 0\n",
    "                p /= p.sum()\n",
    "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "        else:\n",
    "            # 在用GPU(cupy）计算时，优先速度\n",
    "            # 有时目标词存在于负例中\n",
    "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "                                               replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [1 0]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "# 测试negative sampleer的效果\n",
    "corpus = np.array([0,1,2,3,4,1,2,3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1,3,0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为这里没用define by run的架构，所以所有涉及到参数矩阵的计算都需要依托层来实现，因为需要定义反向传播的方法\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 在监督标签为one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None  # sigmoid的输出\n",
    "        self.t = None  # 监督标签\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = (self.y - self.t) * dout / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss:\n",
    "    # sample_size 指的是负采样要采样多少个负例\n",
    "    def __init__(self, W, corpus, power = 0.75, sample_size = 5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "\n",
    "        # 这里生成了sample_size + 1层，因为需要多一个正例的层\n",
    "        # 每一个负例都需要与输出侧的参数矩阵计算并计算loss，最后再把所有负例的loss加起来\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        # loss_layer[0]和embed_dot_layers[0]是处理正例的层，其他是处理负例\n",
    "\n",
    "    def forward(self, h, target):\n",
    "        # target 指的是目标词，除了目标词以外，其他都是负例\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "\n",
    "        # 正例的前向传播\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype = np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "\n",
    "        # 负例的前向传播\n",
    "        # 负例一共有batch_size * sample_size个，但是他们都可以统一用一个label 0\n",
    "        # 因此只需要生成batch_size个negative_label就可以了\n",
    "        negative_label = np.zeros(batch_size, dtype = np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            # h dot W_out -> score\n",
    "            score = self.embed_dot_layers[i + 1].forward(h, negative_target)\n",
    "            # score -> loss\n",
    "            loss += self.loss_layers[i + 1].forward(score, negative_label)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout = 1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            #将dout沿着相反的方向回传，以正向传播相反的顺序调用各层的backward()\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "\n",
    "        return dh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBoW的实现\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 初始化权重\n",
    "        W_in = 0.01*np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01*np.random.randn(V, H).astype('f')\n",
    "\n",
    "        # 生成输入层W_in，一共有2*window_size=context size个输入层\n",
    "        # 多个输入层共享权重\n",
    "        self.in_layers = []\n",
    "        for i in range(2*window_size):\n",
    "            layer = Embedding(W_in)\n",
    "            self.in_layers.append(layer)\n",
    "        \n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power = 0.75, sample_size = 5)\n",
    "\n",
    "        # 将权重整理到列表，注意，这里为什么只出现了in_layer和ns_loss，out_layer去哪里了？\n",
    "        # 我们已经把out_layer(W_out)和loss的计算合并成一个层ns_loss了\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            # 这里的 + 是给列表添加元素，而不是加法运算\n",
    "            # 由于多个输入层共享权重W_in，实际上self.params里面的layer.params都是重复的\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        # W_in就是单词的分布式表示\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "            h *= 1/len(self.in_layers)\n",
    "            loss = self.ns_loss.forward(h, target)\n",
    "            return loss\n",
    "        \n",
    "    def backward(self, dout = 1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1/len(self.in_layers)\n",
    "\n",
    "        # 正向传播中，有2*context_size个context words\n",
    "        # 尽管每个层的grads都是独立初始化的，但每个层在backward时都会把各自的grads累加到共享的W_in上，实际上，每个层的grads都是一样的（正向传播时对多个context words的h取了平均）\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-HW-Py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
