{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== a runnable full training procedure =========\n",
    "# resnet18 + adam ,test acc ~ 99.1%\n",
    "import sys\n",
    "sys.path.append(r'/kaggle/input/dezero')\n",
    "from imp import reload\n",
    "import os, dezero\n",
    "\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "reload(F)\n",
    "import numpy as np\n",
    "\n",
    "from dezero import optimizers\n",
    "reload(optimizers)\n",
    "from dezero import DataLoader\n",
    "# from dezero.models import MLP\n",
    "from dezero import models\n",
    "from dezero.models import ResNet18, MLP\n",
    "from dezero.datasets import Dataset\n",
    "\n",
    "# we can use torch's data loader\n",
    "import torch\n",
    "import torchvision.datasets as tv_datasets\n",
    "import torchvision.transforms as tv_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "kaggle = \"/kaggle/input/cifar-10-dlhw1-2/data\"\n",
    "local = \"../data\"\n",
    "\n",
    "# switch between test and kaggle mode\n",
    "run_on_local = 1\n",
    "\n",
    "if run_on_local:\n",
    "    data_path = local\n",
    "    sys.path.append('./code')\n",
    "    \n",
    "else:\n",
    "    data_path = kaggle\n",
    "    sys.path.append('/kaggle/input')\n",
    "\n",
    "# define data transformation method：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1, 28, 28), (10000,)\n",
      "(60000, 1, 28, 28), (60000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuguy\\AppData\\Local\\Temp\\ipykernel_18744\\1668673348.py:12: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  data = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "# ======= prepare data ========\n",
    "data_path = local\n",
    "batch_size = 64\n",
    "num_workers = 0\n",
    "dataset1 = 'CIFAR10'\n",
    "\n",
    "# load with pickle\n",
    "# we download the mnist dataset and store it in .pkl\n",
    "import pickle\n",
    "# filepath = r'./dezero/MNISTdataset/mnist.pkl'\n",
    "filepath = r'/kaggle/input/dezero/dezero/MNISTdataset/mnist.pkl'\n",
    "with open(filepath, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# setup hyper-params\n",
    "\n",
    "\n",
    "print_every = 1\n",
    "hidden_size = 1000\n",
    "\n",
    "max_epoch = 15\n",
    "batch_size = 64\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "# we only use tv_transform to transform the ndarray data\n",
    "g = tv_transforms.Compose([\n",
    "                tv_transforms.RandomRotation(15),  \n",
    "                tv_transforms.RandomHorizontalFlip(),\n",
    "                tv_transforms.RandomAffine(0, translate=(0.1, 0.1)),  \n",
    "                # tv_transforms.RandomResizedCrop(28, scale=(0.9, 1.1)), \n",
    "                # tv_transforms.ToTensor(),  \n",
    "                tv_transforms.Normalize((0.1307,), (0.3081,))])  \n",
    "\n",
    "def f(x):\n",
    "    return g(torch.tensor(x, dtype=torch.float32))\n",
    "\n",
    "# def f(x):\n",
    "#     # x = x.flatten()\n",
    "#     x = x.astype(np.float32)\n",
    "#     x /= 255.0\n",
    "#     return x\n",
    "\n",
    "test_set = Dataset()\n",
    "test_set.transform = f\n",
    "test_set.data= data['test_img'].reshape(-1, 1, 28, 28)\n",
    "test_set.label = data['test_label']\n",
    "print(f'{test_set.data.shape}, {test_set.label.shape}')\n",
    "\n",
    "\n",
    "train_set = Dataset()\n",
    "train_set.transform = f\n",
    "train_set.data= data['train_img'].reshape(-1, 1, 28, 28)\n",
    "train_set.label = data['train_label']\n",
    "print(f'{train_set.data.shape}, {train_set.label.shape}')\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[226], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     27\u001b[0m acc \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39maccuracy(y, t)\n\u001b[1;32m---> 28\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m sum_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(t)\n\u001b[0;32m     30\u001b[0m sum_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(acc\u001b[38;5;241m.\u001b[39mdata) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(t)\n",
      "File \u001b[1;32md:\\GITrepo\\LLM-from-scratch\\dezero-master\\dezero\\optimizers.py:97\u001b[0m, in \u001b[0;36mAdam.update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\GITrepo\\LLM-from-scratch\\dezero-master\\dezero\\optimizers.py:27\u001b[0m, in \u001b[0;36mOptimizer.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# update params\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# call update_one() method, which will be implemented in child class\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "N = len(train_set)\n",
    "# model = MLP((1000,10))\n",
    "model = ResNet18(pretrained=False)\n",
    "# model = MLP((1000, 10), activation= F.leaky_relu)\n",
    "# Adam works\n",
    "optimizer = optimizers.Adam().setup(model)\n",
    "if dezero.cuda.gpu_enable:\n",
    "    train_loader.to_gpu()\n",
    "    test_loader.to_gpu()\n",
    "    model.to_gpu()\n",
    "\n",
    "\n",
    "# try to load weights (check if path exists)\n",
    "# if os.path.exists('my_mlp.npz'):\n",
    "#     model.load_weights('my_mlp.npz')\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        acc = F.accuracy(y, t)\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data)*len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "\n",
    "    # print training info\n",
    "    avg_loss = sum_loss / len(train_set)\n",
    "    avg_acc = sum_acc / len(train_set)\n",
    "    # if epoch % print_every == print_every-1 :\n",
    "    print(f'epoch: {epoch+1}, train_avg_loss: {avg_loss:.4f}, train_avg_acc: {avg_acc:.4f}')\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    # print eval info\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            # loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            # sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    \n",
    "    # print eval(test) info\n",
    "    # avg_loss = sum_loss / len(test_set)\n",
    "    avg_acc = sum_acc / len(test_set)\n",
    "    # if epoch % print_every == print_every-1 :\n",
    "    # print(f'test_avg_loss: {avg_loss:.4f}, test_avg_acc: {avg_acc:.4f}')\n",
    "    print(f'test_avg_acc: {avg_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize image after transform\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # remove batch dim to get an array of shape (28, 28)\n",
    "# img_arr = data['train_img'][1].reshape(-1,28,28)\n",
    "\n",
    "# img_arr = f(torch.tensor(img_arr, dtype = torch.float32))\n",
    "# image = image_array.squeeze()\n",
    "\n",
    "# # 展示图像\n",
    "# plt.imshow(img_arr.squeeze(), cmap='gray')  # gray projection\n",
    "# plt.axis('off')  # turn off axis\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-HW-Py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
