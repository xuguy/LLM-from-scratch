{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2*np.pi*x) + np.random.rand(100, 1)\n",
    "\n",
    "# initialize weights\n",
    "I, H, O = 1, 10, 1 # input hidden output layers\n",
    "W1 = Variable(0.01 * np.random.randn(I, H))\n",
    "b1 = Variable(np.zeros(H))\n",
    "\n",
    "W2 = Variable(0.01 * np.random.randn(H, O))\n",
    "b2 = Variable(np.zeros(O))\n",
    "\n",
    "def predict(x):\n",
    "    y = F.linear(x, W1, b1)\n",
    "    y = F.sigmoid_simple(y)\n",
    "    y = F.linear(y, W2, b2)\n",
    "    return y\n",
    "\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "    W1.cleargrad()\n",
    "    b1.cleargrad()\n",
    "    W2.cleargrad()\n",
    "    b2.cleargrad()\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data -= lr*W1.grad.data\n",
    "    b1.data -= lr*b1.grad.data\n",
    "    W2.data -= lr*W2.grad.data\n",
    "    b2.data -= lr*b2.grad.data\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)\n",
    "\n",
    "# plot to check how well the nn fit to toy sin() data\n",
    "import matplotlib.pyplot as plt\n",
    "x_t = np.linspace(0,1,100).reshape(100,1)\n",
    "y_pred = predict(x_t).data\n",
    "\n",
    "# pretty well indeed\n",
    "plt.plot(x_t, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Layer` class test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer class\n",
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "from dezero import Parameter\n",
    "from dezero.layers import Layer\n",
    "\n",
    "layer = Layer()\n",
    "layer.p1 = Parameter(np.array(1))\n",
    "layer.p2 = Parameter(np.array(2))\n",
    "layer.p3 = Variable(np.array(3))\n",
    "layer.p4 = 'test'\n",
    "\n",
    "print(layer._params)\n",
    "'''\n",
    "out:\n",
    "{'p2', 'p1'}\n",
    "'''\n",
    "\n",
    "for name in layer._params:\n",
    "    print(name, layer.__dict__[name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Linear类实现神经网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "from dezero import Variable, Layer\n",
    "from dezero.models import Model\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "\n",
    "\n",
    "# toy data\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100,1)\n",
    "y = np.sin(2*np.pi*x) + np.random.rand(100,1)\n",
    "\n",
    "# (old)assign out_size to Linear layer\n",
    "# l1 = L.Linear(10)\n",
    "# l2 = L.Linear(1)\n",
    "\n",
    "# 将Layer作为参数加入Layer的_params中后，我们就可以把作为Layer的Linear实例（L.Linear)传入Layer的实例model的_params中统一管理\n",
    "model = Layer()\n",
    "model.l1 = L.Linear(5)\n",
    "model.l2 = L.Linear(3)\n",
    "\n",
    "# 定义网络结构：\n",
    "def predict(model, x):\n",
    "    # 所有的参数都在model中统一管理\n",
    "    y = model.l1(x)\n",
    "    y = F.sigmoid_simple(y)\n",
    "    y = model.l2(y)\n",
    "    return y\n",
    "\n",
    "# 测试访问所有参数并重置所有参数的梯度：\n",
    "for p in model.params():\n",
    "    #由于params()是yield构造的生成器，因此需要逐个访问\n",
    "    print(p)\n",
    "\n",
    "model.cleargrads()\n",
    "\n",
    "lr = 0.2\n",
    "iters = 1000\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(model, x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    # l1.cleargrads()\n",
    "    # l2.cleargrads()\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    for p in model.params():\n",
    "        p.data -= lr*p.grad.data\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将上面的步骤抽象为一种更便捷的方法：将模型定义为一个继承Layer类的类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TwoLayerNet(Model):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #定义模型的网络结构\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "\n",
    "# toy data: our old friend sin()\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100,1)\n",
    "y = np.sin(2*np.pi*x) + np.random.rand(100,1)\n",
    "\n",
    "hidden_size = 5\n",
    "out_size = 3\n",
    "lr = 0.2\n",
    "iters = 1000\n",
    "model = TwoLayerNet(hidden_size,out_size)\n",
    "model.plot(x)\n",
    "# model.cleargrads()\n",
    "\n",
    "for i in range(iters):\n",
    "    #forward\n",
    "    y_pred = model(x)\n",
    "    # cal loss\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "    #cleargrads before backwards\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    #更新模型参数\n",
    "    for p in model.params():\n",
    "        p.data -= lr*p.grad.data\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试 MLP模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable, Layer\n",
    "from dezero.models import Model, MLP\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "model = MLP((10,20,30,40,1)) #5层（4个hidden1 个out）\n",
    "# 在传入数据前模型不会初始化参数，参数全都是None\n",
    "# next(model.params())\n",
    "\n",
    "# 传入toy data\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100,1)\n",
    "y_pred = model(x)\n",
    "print(y_pred)\n",
    "model._params #{'l0', 'l1', 'l2', 'l3', 'l4'}\n",
    "model.__dict__['l0'].W.shape # (1, 10)\n",
    "\n",
    "\n",
    "for i, w in enumerate(model.params()):\n",
    "    print(f'{i}-{w.shape}: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试__setattr__以及__dicit__的行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%\n",
    "class nametest:\n",
    "    def __init__(self, name='name1'):\n",
    "        self.name = name\n",
    "\n",
    "tmp = nametest()\n",
    "tmp.__dict__ # out: {'name': 'name1'}\n",
    "\n",
    "# 从上面这个例子可以看出，实例tmp的所有变量的值都会被__setattr__以{name:value}的形式存到__dict__中\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Set` 带来的`.params()`遍历取出`parameters`的随机性 \n",
    "\n",
    "- `yield from`的作用是从一个`iter`中生成另一个`iter`对象，嵌套递归，把所有可能的`parameter`逐个取出\n",
    "- 当set中有不同的数据结构时，会产生随机性:每次重启kernel都不一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 你需要用交互模式运行下面的内容\n",
    "- 每次重新启动kernel，`yield from`遍历`iterator`的顺序都是不一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = iter(set([1,3,2,4,'5','6']))\n",
    "next(tmp2)\n",
    "tmp = set([1,3,2,4,'5','6'])\n",
    "\n",
    "def fun(tmp):\n",
    "    for i in tmp:\n",
    "        print(i)\n",
    "        yield i\n",
    "\n",
    "def func(tmp):\n",
    "    for i in tmp:\n",
    "        print(i)\n",
    "        yield from fun(tmp)\n",
    "\n",
    "tmp3 = func(tmp)\n",
    "next(tmp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 `dezero/optimizers.py` 中实现的SGD解决问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable, optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2*np.pi*x) + np.random.rand(100, 1)\n",
    "\n",
    "lr = 0.2\n",
    "max_iter = 1000\n",
    "hidden_size = 10\n",
    "\n",
    "model = MLP((hidden_size, 1))\n",
    "\n",
    "# optimizer = optimizers.SGD(le)\n",
    "optimizer = optimizers.MomentumSGD(lr)\n",
    "optimizer.setup(model)\n",
    "# or equivalently:\n",
    "# optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.update()\n",
    "    if i % 100 == 0:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = next(model.params())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module type (cuda or np) test\n",
    "import numpy as np\n",
    "\n",
    "gx = np.zeros((2,3))\n",
    "isinstance(gx, np.ndarray)\n",
    "assert isinstance(gx, float), 'gy must be np.ndarray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with get_item function\n",
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "y = F.get_item(x, 1)\n",
    "y.backward()\n",
    "print(f'{y}\\n{x.grad}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多次提取同一组元素\n",
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "indices = np.array([0, 0, 1])\n",
    "y = F.get_item(x, indices)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重写Variable类中的__getitem__特殊方法，让get_item函数也可以在Variable类型上使用\n",
    "# 或者直接在dezero.core.py中改写setup_variable\n",
    "Variable.__getitem__ = F.get_item\n",
    "\n",
    "# 行为和np.ndarray的切片一致\n",
    "y = x[1]\n",
    "print(y)\n",
    "y = x[:,2]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import Variable, as_variable\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "def softmaxld(x):\n",
    "    # x = as_variable(x)\n",
    "    y = F.exp(x)\n",
    "    sum_y = F.sum(y)\n",
    "    return y/sum_y\n",
    "\n",
    "model = MLP((10, 3)) # 2 layer fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = Variable(np.array([[0.2, -0.4]]))\n",
    "x = np.array([[0.2, -0.4]])\n",
    "y = model(x)\n",
    "p = softmaxld(y)\n",
    "\n",
    "print(y)\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.backward(retain_grad=True)\n",
    "y.grad # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试softmax与cross_entropy_loss\n",
    "from dezero import Variable, as_variable\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "model = MLP((10, 3))\n",
    "x = np.array([[0.2, -0.4], [0.3, 0.5], [1.3, -3.2], [2.1, 0.3]])\n",
    "\n",
    "t = np.array([2, 0, 1, 0])\n",
    "y = model(x)\n",
    "loss = F.softmax_cross_entropy(y, t)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 Max()定义中的反向传播掩码cond的shape\n",
    "x = Variable(np.random.rand(10,3))\n",
    "y = x.max(axis = 1, keepdims = True)\n",
    "y.shape # shape (10, 1)\n",
    "cond = (x.data == y.data)\n",
    "cond.shape #shape (10, 1) be broadcast to shape (10, 3)\n",
    "cond.ravel().shape\n",
    "\n",
    "x2 = np.random.rand(10,1)\n",
    "x2.shape # (10, 1)\n",
    "x2.ravel().shape #(10, 0)\n",
    "t.ravel().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 螺旋数据集 （step 48）\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import dezero\n",
    "print(f'dezero loaded from: {dezero.__path__}')\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "max_epoch = 100\n",
    "print_every = 10\n",
    "\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "\n",
    "assert print_every >=1 , 'print_every must larger than 1'\n",
    "lr = 1.0\n",
    "\n",
    "# load data\n",
    "# first transform data\n",
    "def f(x):\n",
    "    y = x / 2.0\n",
    "    return y\n",
    "\n",
    "train_set = dezero.datasets.Spiral(transform = f)\n",
    "\n",
    "print(f'train_set shape: {train_set.data.shape}, label shape: {train_set.label.shape}')\n",
    "print(f'data example: {train_set[0]}')\n",
    "\n",
    "# 2 layer fc nn, 3 classes\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(train_set)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "np.random.seed(0)\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    # 随机打乱所有index\n",
    "    \n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # 等距切分打乱后的index\n",
    "        batch_index = index[i*batch_size : (i+1)*batch_size]\n",
    "\n",
    "        # 小批量地读入数据用于训练，防止内存被冲爆\n",
    "        batch = [train_set[i] for i in batch_index]\n",
    "        batch_x = np.array([example[0] for example in batch])\n",
    "        batch_t = np.array([example[1] for example in batch])\n",
    "\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        # 求所有数据的loss总和，并非batch loss\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    avg_loss = sum_loss / data_size\n",
    "    if epoch % print_every == print_every-1 :\n",
    "        print(f'epoch: {epoch+1}, epoch_avg_loss: {avg_loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 Datasets 类\n",
    "import dezero\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train = True)\n",
    "print(train_set[0])\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader class test\n",
    "from dezero.datasets import Spiral\n",
    "from dezero import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "max_epoch = 1\n",
    "\n",
    "train_set = Spiral(train = True)\n",
    "test_set = Spiral(train = False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle = False)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for x, t in train_loader:\n",
    "        print(x.shape, t.shape)\n",
    "        # break: 截断输出，只输出1个用作展示\n",
    "        break\n",
    "    for x, t in test_loader:\n",
    "        print(x.shape, t.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with accuracy\n",
    "import numpy as np\n",
    "import dezero.functions as F\n",
    "\n",
    "y = np.array([[0.2, 0.8, 0. ], [0.1, 0.9, 0], [0.8, 0.1, 0.1]])\n",
    "t = np.array([1, 2, 0])\n",
    "acc = F.accuracy(y, t)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Spiral data to train a MLP\n",
    "import math\n",
    "import numpy as np\n",
    "import dezero\n",
    "import dezero.datasets\n",
    "print(f'dezero loaded from: {dezero.__path__}')\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "max_epoch = 300\n",
    "print_every = 10\n",
    "\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "\n",
    "assert print_every >=1 , 'print_every must larger than 1'\n",
    "lr = 1.0\n",
    "\n",
    "# load data\n",
    "train_set = dezero.datasets.Spiral(train = True)\n",
    "test_set = dezero.datasets.Spiral(train = False)\n",
    "train_loader = dezero.DataLoader(train_set, batch_size)\n",
    "test_loader = dezero.DataLoader(test_set, batch_size, shuffle = False)\n",
    "\n",
    "print(f'train_set shape: {train_set.data.shape}, label shape: {train_set.label.shape}')\n",
    "print(f'data example: {train_set[0]}')\n",
    "\n",
    "# 2 layer fc nn, 3 classes\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "data_size = len(train_set)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    # print training info\n",
    "    avg_loss = sum_loss / data_size\n",
    "    avg_acc = sum_acc / data_size\n",
    "    if epoch % print_every == print_every-1 :\n",
    "        print(f'epoch: {epoch+1}, train_avg_loss: {avg_loss:.2f}, test_avg_acc: {avg_acc:.2f}')\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    # print eval info\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "    \n",
    "    # print eval(test) info\n",
    "    avg_loss = sum_loss / data_size\n",
    "    avg_acc = sum_acc / data_size\n",
    "    if epoch % print_every == print_every-1 :\n",
    "        print(f'test_avg_loss: {avg_loss:.2f}, test_avg_acc: {avg_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "import dezero\n",
    "import os \n",
    "os.chdir(r'./dezero/')\n",
    "train_set = dezero.datasets.MNIST(train = True, transform = None)\n",
    "test_set = dezero.datasets.MNIST(train = False, transform = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with pickle\n",
    "import pickle\n",
    "filepath = r'./dezero/MNISTdataset/mnist.pkl'\n",
    "with open(filepath, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "def f(x):\n",
    "    x = x.flatten()\n",
    "    x = x.astype(np.float32)\n",
    "    x /= 255.0\n",
    "    return x\n",
    "\n",
    "from dezero.datasets import Dataset\n",
    "\n",
    "train_set = Dataset()\n",
    "train_set.transform = f\n",
    "train_set.data= data['train_img'].reshape(-1, 1, 28, 28)\n",
    "train_set.label = data['train_label']\n",
    "print(f'{train_set.data.shape}, {train_set.label.shape}')\n",
    "\n",
    "test_set = Dataset()\n",
    "test_set.transform = f\n",
    "test_set.data= data['test_img'].reshape(-1, 1, 28, 28)\n",
    "test_set.label = data['test_label']\n",
    "print(f'{test_set.data.shape}, {test_set.label.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import dezero\n",
    "import dezero.datasets\n",
    "print(f'dezero loaded from: {dezero.__path__}')\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "model = MLP((1000,1000,10), activation=F.relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, t = train_set[0]\n",
    "print(type(x), x.shape)\n",
    "print(t)\n",
    "plt.imshow(x.reshape(28, 28), cmap = 'gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training a NN\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import dezero\n",
    "import dezero.datasets\n",
    "print(f'dezero loaded from: {dezero.__path__}')\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "# def transform function to preprocess\n",
    "\n",
    "def f(x):\n",
    "    x = x.flatten()\n",
    "    x = x.astype(np.float32)\n",
    "    x /= 255.0\n",
    "    return x\n",
    "\n",
    "# setup hyper-params\n",
    "\n",
    "max_epoch = 5\n",
    "print_every = 1\n",
    "\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "lr = 1.0\n",
    "\n",
    "# 如果不是手动建立数据集（load from .pkl）就把下面两行取消注释\n",
    "# train_set = dezero.datasets.MNIST(train = True, transform = f)\n",
    "# test_set = dezero.datasets.MNIST(train=False, transform = f)\n",
    "\n",
    "train_loader = dezero.DataLoader(train_set, batch_size)\n",
    "test_loader = dezero.DataLoader(test_set, batch_size, shuffle = False)\n",
    "test_set.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "assert print_every >=1 , 'print_every must larger than 1'\n",
    "\n",
    "model = MLP((hidden_size, hidden_size, 10), activation=F.relu)\n",
    "optimizer = optimizers.MomentumSGD().setup(model)\n",
    "\n",
    "# max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    # print training info\n",
    "    avg_loss = sum_loss / len(train_set)\n",
    "    avg_acc = sum_acc / len(train_set)\n",
    "    if epoch % print_every == print_every-1 :\n",
    "        print(f'epoch: {epoch+1}, train_avg_loss: {avg_loss:.4f}, train_avg_acc: {avg_acc:.4f}')\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    # print eval info\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "    \n",
    "    # print eval(test) info\n",
    "    avg_loss = sum_loss / len(test_set)\n",
    "    avg_acc = sum_acc / len(test_set)\n",
    "    if epoch % print_every == print_every-1 :\n",
    "        print(f'test_avg_loss: {avg_loss:.4f}, test_avg_acc: {avg_acc:.4f}')\n",
    "\n",
    "# small modify: test kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数（扁平化）\n",
    "\n",
    "import numpy as np\n",
    "from dezero import Variable, Parameter\n",
    "import dezero.functions as F\n",
    "from dezero.layers import Layer\n",
    "\n",
    "\n",
    "layer = Layer()\n",
    "\n",
    "l1 = Layer()\n",
    "l1.p1 = Parameter(np.array(1))\n",
    "\n",
    "layer.l1 = l1\n",
    "layer.p2 = Parameter(np.array(2))\n",
    "layer.p3 = Parameter(np.array(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {}\n",
    "layer._flatten_params(params_dict)\n",
    "print(params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\GITrepo\\\\LLM-from-scratch\\\\dezero-master'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1, 28, 28), (10000,)\n",
      "(60000, 1, 28, 28), (60000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuguy\\AppData\\Local\\Temp\\ipykernel_22096\\2521429981.py:15: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  data = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "# load saved weights test\n",
    "import os, dezero\n",
    "import dezero.functions as F\n",
    "import numpy as np\n",
    "from dezero import optimizers\n",
    "from dezero import DataLoader\n",
    "# from dezero.models import MLP\n",
    "from dezero.models import ResNet18\n",
    "from dezero.datasets import Dataset\n",
    "\n",
    "# test with pickle\n",
    "import pickle\n",
    "filepath = r'./dezero/MNISTdataset/mnist.pkl'\n",
    "with open(filepath, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# setup hyper-params\n",
    "\n",
    "max_epoch = 5\n",
    "print_every = 1\n",
    "\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "lr = 1.0\n",
    "\n",
    "def f(x):\n",
    "    # x = x.flatten()\n",
    "    x = x.astype(np.float32)\n",
    "    x /= 255.0\n",
    "    return x\n",
    "\n",
    "test_set = Dataset()\n",
    "test_set.transform = f\n",
    "test_set.data= data['test_img'].reshape(-1, 1, 28, 28)\n",
    "test_set.label = data['test_label']\n",
    "print(f'{test_set.data.shape}, {test_set.label.shape}')\n",
    "\n",
    "\n",
    "train_set = Dataset()\n",
    "train_set.transform = f\n",
    "train_set.data= data['train_img'].reshape(-1, 1, 28, 28)\n",
    "train_set.label = data['train_label']\n",
    "print(f'{train_set.data.shape}, {train_set.label.shape}')\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle = False)\n",
    "test_set.data.shape\n",
    "# train_loader.to_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train_set.data.shape\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mimg\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "# train_set.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuguy\\AppData\\Local\\Temp\\ipykernel_26956\\4260935705.py:25: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  data = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1, 28, 28), (10000,)\n",
      "(60000, 1, 28, 28), (60000,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 83\u001b[0m\n\u001b[0;32m     81\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     82\u001b[0m acc \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39maccuracy(y, t)\n\u001b[1;32m---> 83\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m sum_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(t)\n\u001b[0;32m     85\u001b[0m sum_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(acc\u001b[38;5;241m.\u001b[39mdata) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(t)\n",
      "File \u001b[1;32md:\\GITrepo\\LLM-from-scratch\\dezero-master\\dezero\\optimizers.py:97\u001b[0m, in \u001b[0;36mAdam.update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\GITrepo\\LLM-from-scratch\\dezero-master\\dezero\\optimizers.py:27\u001b[0m, in \u001b[0;36mOptimizer.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# update params\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# call update_one() method, which will be implemented in child class\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GITrepo\\LLM-from-scratch\\dezero-master\\dezero\\optimizers.py:125\u001b[0m, in \u001b[0;36mAdam.update_one\u001b[1;34m(self, param)\u001b[0m\n\u001b[0;32m    123\u001b[0m m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1) \u001b[38;5;241m*\u001b[39m (grad \u001b[38;5;241m-\u001b[39m m)\n\u001b[0;32m    124\u001b[0m v \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2) \u001b[38;5;241m*\u001b[39m (grad \u001b[38;5;241m*\u001b[39m grad \u001b[38;5;241m-\u001b[39m v)\n\u001b[1;32m--> 125\u001b[0m param\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust\u001b[49m \u001b[38;5;241m*\u001b[39m m \u001b[38;5;241m/\u001b[39m (xp\u001b[38;5;241m.\u001b[39msqrt(v) \u001b[38;5;241m+\u001b[39m eps)\n",
      "File \u001b[1;32md:\\GITrepo\\LLM-from-scratch\\dezero-master\\dezero\\optimizers.py:99\u001b[0m, in \u001b[0;36mAdam.adjust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madjust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    101\u001b[0m     fix1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt)\n\u001b[0;32m    102\u001b[0m     fix2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load saved weights test\n",
    "# ======== a runnable full training procedure =========\n",
    "from imp import reload\n",
    "import os, dezero\n",
    "\n",
    "import dezero.functions as F\n",
    "reload(F)\n",
    "import numpy as np\n",
    "\n",
    "from dezero import optimizers\n",
    "reload(optimizers)\n",
    "from dezero import DataLoader\n",
    "# from dezero.models import MLP\n",
    "from dezero import models\n",
    "from dezero.models import ResNet18, MLP\n",
    "\n",
    "from dezero.datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "# test with pickle\n",
    "import pickle\n",
    "filepath = r'./dezero/MNISTdataset/mnist.pkl'\n",
    "with open(filepath, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# setup hyper-params\n",
    "\n",
    "max_epoch = 5\n",
    "print_every = 1\n",
    "\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "lr = 1.0\n",
    "\n",
    "def f(x):\n",
    "    x = x.flatten()\n",
    "    x = x.astype(np.float32)\n",
    "    x /= 255.0\n",
    "    return x\n",
    "\n",
    "test_set = Dataset()\n",
    "test_set.transform = f\n",
    "test_set.data= data['test_img'].reshape(-1, 1, 28, 28)\n",
    "test_set.label = data['test_label']\n",
    "print(f'{test_set.data.shape}, {test_set.label.shape}')\n",
    "\n",
    "\n",
    "train_set = Dataset()\n",
    "train_set.transform = f\n",
    "train_set.data= data['train_img'].reshape(-1, 1, 28, 28)\n",
    "train_set.label = data['train_label']\n",
    "print(f'{train_set.data.shape}, {train_set.label.shape}')\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle = False)\n",
    "\n",
    "N = len(train_set)\n",
    "# model = MLP((1000,10))\n",
    "# model = ResNet18(pretrained=False)\n",
    "model = MLP((hidden_size, 10), activation= F.leaky_relu)\n",
    "# Adam works\n",
    "optimizer = optimizers.Adam().setup(model)\n",
    "if dezero.cuda.gpu_enable:\n",
    "    train_loader.to_gpu()\n",
    "    test_loader.to_gpu()\n",
    "    model.to_gpu()\n",
    "\n",
    "\n",
    "# try to load weights (check if path exists)\n",
    "# if os.path.exists('my_mlp.npz'):\n",
    "#     model.load_weights('my_mlp.npz')\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        acc = F.accuracy(y, t)\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data)*len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "\n",
    "    # print training info\n",
    "    avg_loss = sum_loss / len(train_set)\n",
    "    avg_acc = sum_acc / len(train_set)\n",
    "    if epoch % print_every == print_every-1 :\n",
    "        print(f'epoch: {epoch+1}, train_avg_loss: {avg_loss:.4f}, train_avg_acc: {avg_acc:.4f}')\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    # print eval info\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    \n",
    "    # print eval(test) info\n",
    "    avg_loss = sum_loss / len(test_set)\n",
    "    avg_acc = sum_acc / len(test_set)\n",
    "    if epoch % print_every == print_every-1 :\n",
    "        print(f'test_avg_loss: {avg_loss:.4f}, test_avg_acc: {avg_acc:.4f}')\n",
    "\n",
    "# save weights\n",
    "# model.save_weights('my_mlp.npz') # 5.6 MB approx\n",
    "# remove\n",
    "# os.remove('my_mlp.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout test\n",
    "import numpy as np\n",
    "from dezero import test_mode\n",
    "import dezero.functions as F\n",
    "\n",
    "# train mode\n",
    "x = np.ones(5)\n",
    "print(x.shape)\n",
    "y = F.dropout(x, dropout_ratio=0.2)\n",
    "print(y)\n",
    "\n",
    "# test mode\n",
    "with test_mode():\n",
    "    y = F.dropout(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "# 获取卷积输出shape\n",
    "def get_conv_outsize(input_size, kernel_size, stride, pad):\n",
    "    return (input_size + pad*2 - kernel_size) // stride + 1\n",
    "\n",
    "H, W = 4, 4 # input shape\n",
    "KH, KW = 3, 3 # kernel shape\n",
    "SH, SW = 1, 1 # stide\n",
    "PH, PW = 1, 1 # padding\n",
    "\n",
    "OH = get_conv_outsize(H, KH, SH, PH)\n",
    "OW = get_conv_outsize(W, KW, SW, PW)\n",
    "print(OH, OW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage pf np.pad:\n",
    "tmp = np.array([[1,2,3], [4,5,6]])\n",
    "out = np.pad(tmp, ((1,1),(1,1)), mode = 'constant', constant_values=(0,))\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test multi dimension array\n",
    "a = np.random.rand(2,2,3)\n",
    "print(a)\n",
    "b = np.array([[1,2,3], [4,5,6]])\n",
    "a[:,0,:]=b[0:3:1]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test conv2d_simple\n",
    "# import dezero\n",
    "from dezero import Variable\n",
    "import numpy as np\n",
    "from dezero.functions import conv2d_simple\n",
    "from dezero import functions as F\n",
    "\n",
    "N, C, H, W = 1, 5, 15, 15\n",
    "OC, (KH, KW) = 8, (3, 3) # OC: output channel = num of conv kernels (225,45) and (72,5)\n",
    "np.random.seed(0)\n",
    "x = Variable(np.random.randn(N, C, H, W))\n",
    "W = np.random.randn(OC, C, KH, KW)\n",
    "# y = conv2d_simple(x, W)\n",
    "\n",
    "# y = F.conv2dv(x, W, b = None, stride = 1, pad = 1)\n",
    "y = F.conv2d(x, W, b = None, stride = 1, pad = 1)\n",
    "# print(y)\n",
    "# print(y2)\n",
    "y.backward()\n",
    "# y2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'cupy'\n",
      "<class 'numpy.ndarray'> (1, 3, 224, 224)\n",
      "340\n"
     ]
    }
   ],
   "source": [
    "# vgg\n",
    "from dezero.models import VGG16\n",
    "# from dezero.models import ResNet50\n",
    "import numpy as np\n",
    "import dezero\n",
    "\n",
    "from PIL import Image\n",
    "np.random.seed(0)\n",
    "img_path = r'zebra.jpg'\n",
    "img = Image.open(img_path)\n",
    "# img.show()\n",
    "img = img.convert('RGB')\n",
    "img = img.resize((224, 224))\n",
    "img = np.asarray(img, dtype = np.float32)\n",
    "img = img[:,:,::-1]\n",
    "img -= np.array([103.939, 116.779, 123.68], dtype = np.float32)\n",
    "x = img.transpose((2, 0, 1))\n",
    "# x = VGG16.preprocess(img)\n",
    "x = x[np.newaxis]\n",
    "print(type(x), x.shape)\n",
    "\n",
    "# model = ResNet50(pretrained=True)\n",
    "# model = ResNet50(pretrained=True)\n",
    "model = VGG16(pretrained=True)\n",
    "with dezero.test_mode():\n",
    "    y = model(x)\n",
    "predict_id = np.argmax(y.data)\n",
    "print(predict_id)\n",
    "\n",
    "\n",
    "t = np.array(1)\n",
    "\n",
    "\n",
    "# check labels: https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/\n",
    "# 340 = zebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dezero.functions as F\n",
    "from dezero import optimizers\n",
    "optimizer = optimizers.SGD().setup(model)\n",
    "t = t[np.newaxis]\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([[ 2.68649794e-12 -1.00000000e+00  8.48579887e-13  6.74720804e-13\n",
       "            4.55146066e-13  1.13109554e-13  4.07790121e-14  1.01495634e-09\n",
       "            1.21575847e-10  1.16239562e-05  1.32680238e-11  2.18180179e-11\n",
       "            3.04128320e-11  1.47452135e-12  1.60467160e-11  3.71029206e-12\n",
       "            1.95480906e-13  3.61300334e-12  2.05427828e-11  1.67322113e-12\n",
       "            1.59503040e-13  2.97610936e-09  6.25343527e-12  2.21849383e-08\n",
       "            6.76795564e-10  8.10616358e-14  2.41847991e-13  7.59049602e-15\n",
       "            2.96427433e-13  2.12781398e-13  4.33394606e-14  5.41618447e-15\n",
       "            7.09959164e-14  2.77280613e-13  7.44439059e-12  1.96565895e-13\n",
       "            7.45122280e-12  4.89351235e-13  2.06801330e-12  6.19733709e-10\n",
       "            3.94560585e-14  4.71847977e-11  1.19278350e-12  1.18897448e-12\n",
       "            2.78879242e-13  5.56105075e-12  5.77999156e-14  5.39745505e-12\n",
       "            1.39073406e-11  2.71652006e-10  3.69585525e-12  4.10928402e-09\n",
       "            1.58017154e-13  1.57010106e-13  5.75548452e-13  2.30475801e-15\n",
       "            2.42883025e-13  2.33498268e-12  2.98357910e-12  1.18512933e-13\n",
       "            1.19381623e-13  3.31509906e-12  1.25156821e-11  6.06214688e-12\n",
       "            7.98658015e-14  2.53129386e-13  7.16264157e-12  1.53572684e-12\n",
       "            5.34252547e-12  8.72296766e-12  2.30307283e-13  4.00805777e-12\n",
       "            6.20931691e-12  6.47780163e-11  1.93088757e-11  1.22432934e-13\n",
       "            6.00135983e-11  2.12699541e-12  8.63723643e-13  5.60521161e-12\n",
       "            1.18906007e-09  1.69715162e-11  3.55949368e-08  1.27940064e-06\n",
       "            1.41706746e-09  8.50170434e-10  2.09455084e-08  1.42089790e-12\n",
       "            2.01715190e-11  2.53081258e-12  7.18845068e-13  1.25415456e-10\n",
       "            8.42846209e-11  1.01691489e-10  1.00112384e-11  4.82028832e-15\n",
       "            1.75235390e-12  3.10670240e-11  8.67764411e-13  2.30821442e-07\n",
       "            2.69420292e-10  7.90063215e-09  1.00961350e-10  6.89528458e-14\n",
       "            5.94989480e-09  7.25976842e-13  2.10523656e-12  1.41035935e-12\n",
       "            4.87485269e-13  1.40309558e-13  2.85989873e-12  2.13414525e-14\n",
       "            1.08903763e-11  4.97863209e-12  1.86488161e-14  3.54071841e-11\n",
       "            4.12433596e-13  3.62791394e-11  6.91718953e-14  6.19750223e-14\n",
       "            7.03752768e-12  6.47952219e-14  5.09004629e-14  6.11619913e-12\n",
       "            9.31480240e-12  2.93658761e-13  1.34599504e-12  4.51376547e-10\n",
       "            4.81154561e-10  2.72493195e-10  4.71781614e-10  2.13129001e-10\n",
       "            1.54253041e-10  3.80926179e-09  1.17876823e-06  3.64613950e-10\n",
       "            6.19242213e-10  2.50161669e-10  2.70638884e-06  1.73354543e-11\n",
       "            3.05744818e-10  1.01525011e-10  9.55968860e-10  6.99971331e-11\n",
       "            1.23711330e-09  1.82911000e-12  5.74508149e-11  1.46860673e-13\n",
       "            7.02033646e-13  3.44073755e-13  2.80827661e-12  4.40691580e-11\n",
       "            2.23414440e-13  2.88083122e-14  1.17744613e-13  6.31569124e-13\n",
       "            3.15529287e-12  5.96229142e-12  1.97475022e-12  1.07243846e-10\n",
       "            7.30158989e-11  2.11593677e-12  3.11535682e-13  7.19635948e-11\n",
       "            2.06084021e-11  4.02947641e-11  2.98514425e-12  1.07665848e-10\n",
       "            3.86503156e-12  6.02851588e-11  7.59717844e-10  2.27436472e-11\n",
       "            9.76938197e-10  2.13592949e-10  3.58023333e-12  1.14027524e-10\n",
       "            1.47301202e-08  1.08415019e-08  7.11453063e-10  6.34307168e-13\n",
       "            1.75487697e-11  2.50183485e-11  3.37771824e-12  2.62383377e-13\n",
       "            1.86316015e-11  7.55624383e-12  2.08868721e-11  3.23010785e-11\n",
       "            4.39723258e-12  3.61042012e-12  8.94103693e-13  3.67377309e-12\n",
       "            2.10357981e-11  2.39633313e-10  7.97892568e-13  3.04185019e-12\n",
       "            1.31008377e-12  3.90597173e-11  9.03788069e-12  7.16510141e-12\n",
       "            5.20230359e-10  2.08547450e-11  3.12951444e-12  5.10718724e-12\n",
       "            8.90889264e-14  8.05170819e-10  1.80603310e-11  1.26425870e-10\n",
       "            4.02566008e-10  6.64003783e-11  3.49682699e-10  2.87953914e-11\n",
       "            3.19842153e-09  1.36437354e-11  6.84655999e-10  1.75002429e-10\n",
       "            1.17517523e-10  2.22575517e-11  5.36931298e-11  5.92048676e-12\n",
       "            6.21700825e-12  5.45258500e-12  6.24422528e-11  4.08009217e-12\n",
       "            7.07092648e-11  9.09972375e-11  2.97457441e-11  3.08463310e-10\n",
       "            1.89492432e-11  1.35279234e-12  1.46142126e-11  3.11682347e-10\n",
       "            4.25256136e-10  2.70794186e-11  2.78026685e-10  4.53280434e-11\n",
       "            6.14147231e-12  1.47708007e-12  3.76085239e-11  1.37095257e-11\n",
       "            1.82386450e-11  3.28222657e-11  6.44779456e-12  6.91197045e-13\n",
       "            1.15341601e-12  2.09503790e-12  6.00547112e-11  7.88399836e-13\n",
       "            2.76312098e-11  1.85161192e-10  9.13817546e-12  3.56776408e-09\n",
       "            6.52261795e-12  1.93485054e-11  1.78988020e-12  1.51024106e-11\n",
       "            1.29560225e-11  1.65039146e-11  1.07779003e-11  1.65718502e-12\n",
       "            9.08126547e-13  5.71475918e-13  2.61632113e-14  5.27913477e-12\n",
       "            3.66748784e-11  4.05708073e-13  5.25313435e-14  1.01995634e-11\n",
       "            2.56387168e-11  1.85324436e-10  1.71033222e-11  4.59084104e-10\n",
       "            1.91323100e-08  1.38014905e-10  2.71257106e-09  6.74633753e-08\n",
       "            1.45078729e-07  1.25211827e-10  6.08467221e-10  7.41119666e-12\n",
       "            1.01596553e-09  5.20503987e-11  1.93171914e-08  3.47542972e-14\n",
       "            3.05985895e-13  3.13324922e-11  3.89496838e-11  9.59479371e-11\n",
       "            4.14797779e-10  1.42635025e-12  3.18802797e-11  3.23044841e-10\n",
       "            1.99931961e-07  8.43833519e-08  4.90829821e-10  2.86024919e-11\n",
       "            7.49736590e-13  5.84029700e-11  3.50954932e-10  1.32896272e-11\n",
       "            4.46490284e-12  2.29306618e-13  1.98781443e-12  4.38793018e-13\n",
       "            2.01272722e-12  1.96477071e-13  1.91629034e-13  2.11982359e-11\n",
       "            8.58961014e-13  9.47546078e-13  7.20446923e-13  2.61640839e-12\n",
       "            9.01305679e-12  8.97662614e-14  1.72623612e-12  1.13068578e-13\n",
       "            1.36756698e-12  1.02603134e-13  7.64270212e-13  1.21444055e-12\n",
       "            1.71659104e-14  5.78981172e-14  1.00440728e-12  8.98240596e-12\n",
       "            7.81303733e-14  7.86689981e-14  1.44178517e-12  5.96920381e-14\n",
       "            1.20653406e-14  7.45356061e-14  3.15690217e-11  1.51457180e-10\n",
       "            8.80338825e-14  1.14233658e-14  3.15205674e-12  9.98184933e-13\n",
       "            1.37585819e-12  3.03801324e-11  5.10566939e-15  4.31113589e-09\n",
       "            9.99679208e-01  6.09076034e-09  2.00914130e-08  3.01074761e-06\n",
       "            2.21250485e-09  8.39168457e-10  8.99737600e-08  2.18528473e-10\n",
       "            1.24065611e-08  3.30455485e-09  9.40540179e-09  7.19462150e-06\n",
       "            2.56415457e-04  3.60803715e-05  1.33598181e-08  4.29270841e-08\n",
       "            2.66901562e-12  6.78766418e-12  4.36370964e-13  6.85433643e-13\n",
       "            1.47055274e-12  2.62464328e-10  3.28622927e-11  5.04924214e-10\n",
       "            5.97715941e-13  1.13622078e-12  1.48079791e-12  4.36347052e-12\n",
       "            8.50594508e-12  3.26226095e-11  4.34878088e-12  1.20842336e-09\n",
       "            2.16542384e-09  3.54329044e-12  2.61075328e-10  1.55702645e-10\n",
       "            1.13171174e-11  2.74880514e-13  7.62541777e-13  9.71739269e-12\n",
       "            4.63592402e-12  5.94126970e-11  1.14148838e-12  7.19515503e-10\n",
       "            2.31656569e-11  1.14954296e-10  2.15315179e-08  5.88490736e-13\n",
       "            1.39472199e-14  1.07677903e-11  7.03488984e-14  1.34695293e-12\n",
       "            7.68586909e-13  6.36345021e-14  1.83162708e-12  3.85251206e-12\n",
       "            3.90940974e-10  7.37910330e-13  2.40973257e-12  1.06445896e-12\n",
       "            7.17227249e-14  6.25166169e-11  3.83073347e-13  6.81737898e-15\n",
       "            4.95975873e-13  8.42111049e-13  1.17029718e-13  8.15839312e-13\n",
       "            2.06295189e-11  5.21931775e-14  1.66921199e-11  1.49602303e-12\n",
       "            4.84149006e-11  1.09383103e-09  2.44454522e-12  7.65071939e-14\n",
       "            3.08904118e-11  5.68423142e-10  1.16021572e-13  1.97483696e-12\n",
       "            2.64915314e-13  8.17609869e-11  9.46075764e-14  7.72256174e-13\n",
       "            3.21236500e-14  2.54127136e-11  9.77560589e-14  4.36697761e-11\n",
       "            2.67445066e-10  5.15100419e-12  1.55060912e-11  1.55244253e-13\n",
       "            6.61542978e-13  1.80655588e-12  1.34908198e-12  1.84570046e-12\n",
       "            6.11172950e-13  1.75908160e-12  2.79265001e-14  2.47751537e-12\n",
       "            1.04933423e-12  5.64137886e-12  3.40760596e-13  6.48788477e-13\n",
       "            1.00620623e-11  3.97146942e-12  6.84111893e-13  8.54277551e-11\n",
       "            1.39738499e-10  2.06693759e-11  2.49276172e-14  2.92761719e-13\n",
       "            7.28483409e-13  9.09799105e-14  1.54666872e-15  2.06212760e-13\n",
       "            1.17956755e-10  2.13893465e-13  1.00443590e-12  5.05084310e-13\n",
       "            4.15148818e-12  5.33164355e-12  1.64974848e-12  2.69715795e-11\n",
       "            4.14420971e-12  2.64479736e-12  4.53976033e-15  1.40513632e-13\n",
       "            7.80163084e-14  5.95209949e-12  2.25659430e-13  2.17474302e-11\n",
       "            1.53286273e-11  1.28931737e-13  2.91894140e-13  5.98306466e-14\n",
       "            1.62346629e-11  1.32995340e-12  1.13407938e-12  1.47696105e-13\n",
       "            6.67591796e-13  1.55434530e-12  1.26896096e-12  2.74854531e-11\n",
       "            2.82214492e-13  2.39476086e-14  1.65644164e-13  1.82473597e-13\n",
       "            5.79864498e-13  1.08808784e-09  2.63567727e-12  4.01135993e-13\n",
       "            1.54146108e-12  2.63122586e-13  2.35159687e-11  1.07611084e-13\n",
       "            1.27594292e-13  3.75097869e-12  1.62221704e-13  9.45886707e-13\n",
       "            7.77678685e-11  1.78769499e-10  1.57770292e-12  1.83065698e-11\n",
       "            6.30514183e-11  3.35560762e-13  7.59506291e-10  4.23914349e-13\n",
       "            9.82191962e-14  2.37844900e-12  1.86170185e-13  3.03747607e-14\n",
       "            9.82818948e-12  1.82805919e-13  4.06287892e-10  1.28198658e-11\n",
       "            2.13202784e-12  5.11913157e-13  6.72784571e-12  3.86669619e-13\n",
       "            2.12110060e-12  9.16878352e-15  6.23521582e-11  2.66273201e-11\n",
       "            7.99022632e-13  8.31416880e-12  1.77469408e-13  5.07700869e-13\n",
       "            3.28064588e-13  1.13004604e-12  6.75428951e-13  6.60093562e-13\n",
       "            7.59788066e-12  1.18560706e-12  8.22659608e-14  1.07818587e-13\n",
       "            4.65332287e-12  1.99140912e-11  2.28407094e-12  5.72495133e-12\n",
       "            1.14664842e-13  2.19957971e-12  8.10651866e-14  7.57023404e-13\n",
       "            2.73411450e-11  1.01829580e-12  7.58062179e-13  5.14984214e-13\n",
       "            7.87518651e-14  1.27892515e-13  3.39017334e-13  1.37358142e-13\n",
       "            5.52820830e-14  4.46165110e-12  1.08928849e-12  6.20988547e-12\n",
       "            4.14341911e-12  3.59313246e-10  1.81964166e-12  2.12888978e-13\n",
       "            1.74982962e-11  2.41779741e-13  1.38284922e-11  1.57226188e-13\n",
       "            2.65932865e-13  3.04228691e-11  1.31970704e-12  5.68445031e-13\n",
       "            7.59879030e-13  2.00407555e-14  1.91933736e-13  3.37116430e-13\n",
       "            4.77771572e-11  4.82152458e-13  3.06735713e-12  1.34267519e-11\n",
       "            4.59475253e-12  8.85620678e-13  2.48473578e-13  2.94243525e-13\n",
       "            2.18560326e-12  1.55397552e-11  6.44419820e-14  3.84392984e-13\n",
       "            2.90233788e-12  1.66251648e-12  3.89679643e-11  3.64952914e-13\n",
       "            1.47529504e-12  4.18742352e-13  3.88063605e-14  4.36354884e-14\n",
       "            5.62667182e-14  2.48856946e-12  9.96812767e-14  2.38245014e-11\n",
       "            7.38945884e-12  4.08435628e-13  3.61157718e-12  5.76468257e-12\n",
       "            2.91839330e-11  8.72244768e-13  7.97130536e-13  2.32084324e-10\n",
       "            8.66739232e-12  1.23154999e-13  1.21259704e-14  4.78651298e-13\n",
       "            6.13126694e-12  1.02568273e-10  6.98538536e-12  8.73062872e-11\n",
       "            1.66958610e-13  5.28971788e-12  2.65601994e-12  2.39644502e-12\n",
       "            2.45769036e-13  4.88304167e-13  6.99846778e-12  9.18829466e-13\n",
       "            3.83677552e-12  6.98900104e-11  7.93569718e-14  4.71207166e-13\n",
       "            4.60742338e-13  3.04786123e-12  6.52100032e-12  5.74843856e-14\n",
       "            1.58253716e-12  2.51349776e-13  9.64578505e-15  7.69726486e-14\n",
       "            5.00595821e-13  8.09246130e-14  1.29205555e-12  6.60876122e-16\n",
       "            3.84885483e-12  4.58293854e-11  4.59432275e-12  1.12843762e-11\n",
       "            3.17305448e-12  4.92559639e-12  1.03600407e-12  1.08504429e-11\n",
       "            4.73795699e-12  3.05234072e-12  4.34734631e-08  3.04207506e-13\n",
       "            6.68511402e-14  4.18639515e-12  3.59626890e-12  1.37761940e-13\n",
       "            3.38821159e-11  4.79044278e-12  1.37970007e-12  1.04980321e-11\n",
       "            3.27593963e-12  7.47411708e-14  1.84467892e-13  4.45078002e-12\n",
       "            1.28200124e-11  1.20522657e-12  1.85507005e-13  3.68330262e-11\n",
       "            3.75702724e-13  3.33362817e-11  1.77494889e-12  1.66110832e-13\n",
       "            9.14941950e-13  8.31597572e-16  1.60316584e-12  1.99526146e-10\n",
       "            3.20941516e-13  4.90511596e-14  1.41081756e-11  5.50679511e-13\n",
       "            7.85032372e-10  4.70188179e-13  3.84475817e-13  2.40002202e-12\n",
       "            5.72685684e-14  3.04819277e-12  1.96451432e-11  5.41964260e-13\n",
       "            2.91048002e-13  4.18385063e-15  3.35525886e-15  2.00597338e-13\n",
       "            5.16892132e-14  1.05368839e-12  2.31991815e-10  2.76217393e-14\n",
       "            1.06095403e-13  4.79060758e-12  7.02656986e-12  1.60425774e-12\n",
       "            3.02622971e-13  3.48180767e-13  7.55784411e-12  6.44572893e-12\n",
       "            1.96669347e-10  9.47332143e-11  2.19020635e-12  4.93392527e-09\n",
       "            1.33937501e-12  1.39219853e-12  1.52692147e-11  7.28672385e-13\n",
       "            1.84222401e-10  4.94240174e-13  2.10738084e-13  8.43131608e-13\n",
       "            2.37741210e-14  2.68319329e-13  6.33558418e-14  2.83175881e-13\n",
       "            1.26774719e-11  6.19301539e-13  2.68432776e-11  7.08077097e-13\n",
       "            2.89369409e-15  2.85996522e-11  1.55239808e-13  7.28022590e-12\n",
       "            2.45089105e-11  1.54018655e-11  3.01111437e-14  1.21220276e-12\n",
       "            1.01818901e-12  1.10089793e-11  9.96074156e-09  1.14256287e-11\n",
       "            5.71070589e-13  4.47026666e-10  1.68556778e-14  7.47014950e-12\n",
       "            1.34297811e-14  6.26533493e-13  1.93493173e-11  6.02323421e-13\n",
       "            5.21682480e-12  9.78562934e-13  3.11893550e-13  2.38587318e-12\n",
       "            3.28545486e-13  3.69810980e-13  5.02226353e-13  2.64718412e-12\n",
       "            1.83342013e-12  1.09570859e-12  4.93693020e-12  3.32917016e-13\n",
       "            6.50009425e-14  1.38748336e-11  9.40105957e-13  4.97465480e-12\n",
       "            2.55407587e-11  5.18754692e-11  3.23340259e-13  3.86446009e-14\n",
       "            9.08953143e-13  2.48604902e-13  1.58623906e-12  1.00089598e-12\n",
       "            7.86939292e-10  1.80437804e-10  5.80757718e-13  3.51220274e-13\n",
       "            4.72554764e-12  9.15264717e-13  1.93113667e-12  4.87564023e-14\n",
       "            3.49143197e-12  1.35977948e-12  1.87509066e-13  1.41595841e-11\n",
       "            1.75596638e-13  5.72660842e-13  3.51003745e-14  3.38867059e-11\n",
       "            1.63031914e-13  1.28540945e-11  7.39264173e-13  5.73210793e-12\n",
       "            8.93717880e-14  7.85726820e-15  2.30045878e-11  1.54944148e-11\n",
       "            4.86537521e-14  3.25879948e-13  1.52119515e-11  1.11007861e-11\n",
       "            2.07625479e-11  2.07584601e-12  6.20876050e-12  6.67279613e-14\n",
       "            6.51360950e-14  4.51260326e-14  2.54436055e-15  4.54925648e-13\n",
       "            3.65635950e-12  8.80469810e-14  2.27013232e-13  5.26861812e-14\n",
       "            2.77602507e-12  1.33256964e-11  9.48283878e-13  2.27625888e-13\n",
       "            3.80755670e-12  3.63945663e-13  8.49013243e-14  1.51407844e-13\n",
       "            1.72795231e-12  4.74838155e-11  4.81492667e-13  1.00395292e-09\n",
       "            4.37561906e-13  1.27516433e-13  1.64152166e-12  5.65353394e-14\n",
       "            5.79140522e-13  3.12774784e-12  8.53422828e-14  7.44655209e-15\n",
       "            6.84200689e-13  1.72585932e-10  1.12735444e-13  7.65129171e-13\n",
       "            3.31662302e-12  2.55761720e-13  4.81621722e-11  8.05349711e-12\n",
       "            1.33691773e-11  1.06061470e-12  5.70805935e-13  8.58382918e-13\n",
       "            5.16129153e-12  3.84879038e-11  9.10504918e-12  8.06756029e-13\n",
       "            5.42362596e-12  4.20964201e-11  1.72984942e-11  1.04723241e-09\n",
       "            4.73356727e-12  3.09671112e-14  8.44651432e-12  4.09730323e-12\n",
       "            2.74576558e-13  1.00754873e-11  4.04952527e-14  1.18776614e-12\n",
       "            2.03010718e-13  1.24941203e-11  4.58368586e-13  1.35677168e-12\n",
       "            3.28104349e-11  2.86996945e-12  1.03293298e-10  2.60171355e-12\n",
       "            5.19591660e-14  3.08142509e-12  2.46529294e-12  6.37019049e-10\n",
       "            5.21237871e-12  1.17137352e-13  3.82393589e-10  8.40316428e-11\n",
       "            5.24629081e-12  1.68156830e-12  1.17510667e-12  6.12360572e-14\n",
       "            1.04333764e-11  4.88552211e-12  3.61984206e-14  1.26601887e-12\n",
       "            1.88392097e-12  7.73881122e-13  4.86201270e-12  5.94809358e-12\n",
       "            1.45521650e-10  4.10721479e-13  4.48820153e-13  4.03939295e-12\n",
       "            9.98371633e-13  4.53519774e-12  9.55894999e-12  2.60801563e-14\n",
       "            8.14916398e-11  1.98639201e-13  1.38227624e-12  1.15708197e-13\n",
       "            9.47449855e-14  6.74511011e-13  6.95894080e-14  8.35783970e-13\n",
       "            2.02264854e-12  6.94721104e-12  5.51681660e-12  1.18291947e-11\n",
       "            1.77303880e-10  7.93340003e-13  1.02906182e-12  1.31467157e-13\n",
       "            9.63299861e-13  1.37493207e-12  2.67150820e-14  1.44599339e-12\n",
       "            6.93875567e-14  2.80586440e-14  7.51482210e-12  3.72647009e-12\n",
       "            8.44708803e-10  6.90189659e-12  8.06487920e-14  1.28885878e-11\n",
       "            7.26119945e-12  1.52532639e-11  5.80138558e-14  2.48916773e-12\n",
       "            5.15731338e-13  8.09271660e-12  2.18189941e-13  5.55451215e-15\n",
       "            1.51408427e-13  1.13147694e-14  2.09045254e-15  8.66475538e-14\n",
       "            6.69981919e-14  1.49078753e-12  5.24016213e-15  1.46334649e-14\n",
       "            1.57441432e-12  5.53574952e-15  9.61416141e-14  1.12415019e-14\n",
       "            1.07782144e-10  3.60223800e-14  1.55425054e-12  9.58230162e-15\n",
       "            1.90378278e-14  3.14119522e-15  6.42428073e-14  1.06204464e-14\n",
       "            2.13090275e-11  3.53672890e-15  1.44543502e-10  7.96830824e-10\n",
       "            9.87291846e-13  7.12506836e-14  3.91453498e-14  1.18945001e-13\n",
       "            2.03876412e-13  5.03722701e-14  1.04706510e-12  7.77586664e-15\n",
       "            3.34358061e-15  7.57487673e-15  4.08596823e-10  6.67627168e-14\n",
       "            3.76474763e-12  4.62538197e-14  4.38014078e-15  8.78235269e-15\n",
       "            8.35620618e-15  4.44523314e-15  1.68349484e-11  8.13733629e-13\n",
       "            4.46571989e-12  4.22079125e-12  4.11883505e-12  2.17682513e-12\n",
       "            2.32246622e-12  1.18994597e-11  8.14003704e-13  7.46961437e-10\n",
       "            4.77734909e-12  3.84675763e-11  4.14743795e-11  5.85657148e-11\n",
       "            2.55772376e-10  4.03142138e-11  2.15695680e-13  1.42114375e-13\n",
       "            5.54970847e-10  2.93393239e-14  2.19514443e-13  6.43816320e-11\n",
       "            3.52709420e-12  1.00425682e-10  3.75749135e-14  2.43885858e-12\n",
       "            2.74765822e-12  2.31461985e-13  3.24614264e-14  1.07875054e-11\n",
       "            2.26885784e-11  1.65673783e-14  1.68590784e-11  1.89985224e-12]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model(x)\n",
    "loss = F.softmax_cross_entropy(y, t)\n",
    "model.cleargrads()\n",
    "loss.backward(retain_grad=True)\n",
    "y.grad\n",
    "# optimizer.update()\n",
    "# sum_loss += float(loss.data)*len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'memoryview'>\n",
      "<class 'memoryview'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(optimizer.target.params()):\n",
    "    print(type(i.grad.data) if i.grad is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testint pooling\n",
    "from dezero.functions import pooling\n",
    "from dezero import Variable\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "tmp = Variable(np.random.rand(1, 1, 4, 4))\n",
    "print(tmp)\n",
    "tmp2 = pooling(tmp, kernel_size=2, stride = 2)\n",
    "print(tmp2)\n",
    "\n",
    "tmp2.backward(create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn test\n",
    "import numpy as np\n",
    "import dezero.layers as L\n",
    "\n",
    "rnn = L.RNN(10)\n",
    "x = np.random.rand(1,1)\n",
    "h = rnn(x)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple rnn network: rnn + fc\n",
    "from dezero import Model\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "\n",
    "class SimpleRNN(Model):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.rnn = L.RNN(hidden_size)\n",
    "        self.fc = L.Linear(out_size)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn.reset_state()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.rnn(x)\n",
    "        y = self.fc(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test simplernn network:\n",
    "import numpy as np\n",
    "\n",
    "seq_data = [np.random.randn(1, 1) for _ in range(1000)]\n",
    "xs = seq_data[0:-1]\n",
    "ts = seq_data[1:] # xs的下一个时间点的数据，ts相当于输入值xs的标签\n",
    "\n",
    "# hidden size = 10, out_size = 1\n",
    "model = SimpleRNN(10, 1)\n",
    "\n",
    "loss, cnt = 0, 0\n",
    "for x, t in zip(xs, ts):\n",
    "    y = model(x)\n",
    "    loss += F.mean_squared_error(y, t)\n",
    "    cnt += 1\n",
    "    # 在第二个输入数据到来时bp\n",
    "    if cnt==2:\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reload test2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "variable([ 1.  -0.2  2.   0. ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload test\n",
    "\n",
    "from imp import reload\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "from dezero import Variable\n",
    "import numpy as np\n",
    "\n",
    "reload(F)\n",
    "x = np.array([1,-1,2.,0])\n",
    "F.leaky_relu(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 32, 32])\n",
      "torch.Size([3, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# auto-adaptive to in_channel test\n",
    "'''\n",
    "我们无法define in_channels in runtime，必须事先指定in_channels\n",
    "torch.nn.Conv2d(\n",
    "    in_channels,\n",
    "    out_channels,\n",
    "    kernel_size,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    groups=1,\n",
    "    bias=True,\n",
    "    padding_mode=\"zeros\",\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "用nn.LazyConv2d insetead:\n",
    "torch.nn.LazyConv2d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "conv2 = nn.LazyConv2d(16, 3, padding = 1)\n",
    "x1 = torch.tensor(np.random.rand(1,3,32,32), dtype=torch.float32)\n",
    "x2 = torch.tensor(np.random.rand(3,3,32,32), dtype=torch.float32)\n",
    "# print(x.shape)\n",
    "out1 = conv2(x1)\n",
    "out2 = conv2(x2)\n",
    "print(out1.shape)\n",
    "print(out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9645643778539267\n",
      "variable([ 0.40376765 -0.14084374])\n"
     ]
    }
   ],
   "source": [
    "# test sigmoid focal loss\n",
    "from imp import reload\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "reload(F)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 模拟输入和标签\n",
    "x = Variable(np.array([2.0, -3.0], dtype=np.float32))\n",
    "target = Variable(np.array([0.0, 1.0], dtype=np.float32))\n",
    "\n",
    "# 计算 Focal Loss\n",
    "loss = F.sigmoid_focal_loss(x, target, alpha=0.25, gamma=2)\n",
    "print(\"Loss:\", loss.data)  # 输出每个样本的损失值\n",
    "x.cleargrad()\n",
    "loss.backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9646, grad_fn=<MeanBackward0>)\n",
      "tensor([ 0.4038, -0.1408])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.ops import sigmoid_focal_loss as focal\n",
    "import torch\n",
    "x = torch.tensor(np.array([2.0, -3.0], dtype=np.float32), requires_grad=True)\n",
    "target = torch.tensor(np.array([0.0, 1.0], dtype=np.float32), requires_grad=False)\n",
    "loss_torch = focal(x, target, reduction='mean')\n",
    "print(loss_torch)\n",
    "\n",
    "loss_torch.backward()\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-HW-Py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
